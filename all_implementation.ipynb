{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Hl2S3WU9kdZB"},"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7420,"status":"ok","timestamp":1686977636084,"user":{"displayName":"Minghao Zhu","userId":"16328474558609043768"},"user_tz":-480},"id":"mZufbXL8mThp"},"outputs":[],"source":["import os\n","import numpy as np\n","import tqdm\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import resnet.resnet as resnet\n","from resnet.resnet_utils import myResnet\n","import os\n","import random\n","\n","from torchvision import transforms\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QjLyV_WJnEwm"},"source":["# Tokenizer"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686977636084,"user":{"displayName":"Minghao Zhu","userId":"16328474558609043768"},"user_tz":-480},"id":"8uDACPwDmxly"},"outputs":[],"source":["from typing import List\n","class Tokenizer():\n","  def __init__(self, ):\n","    self.word2idx = {}\n","    self.idx2word = {}\n","    self.idx = 3\n","    self.word2idx['<e>'] = 1\n","    self.word2idx['</e>'] = 2\n","    self.idx2word[1] = '<e>'\n","    self.idx2word[2] = '</e>'\n","\n","  def fit(self, text):\n","    for word in text.split():\n","        if word not in self.word2idx:\n","            self.word2idx[word] = self.idx\n","            self.idx2word[self.idx] = word\n","            self.idx += 1\n","\n","  @classmethod\n","  def pad_sequences(cls, lst_seq: List[List[int]], pad_value=0, pad_left=False, pad_to: int=None) -> List[List[int]]:\n","    maxlen = max(len(seq) for seq in lst_seq) if pad_to is None else pad_to\n","    if pad_left:\n","        padded_seq = [[pad_value] * (maxlen - len(seq)) + seq for seq in lst_seq]\n","    else:\n","        padded_seq = [seq + [pad_value] * (maxlen - len(seq)) for seq in lst_seq]\n","    return padded_seq\n","\n","  def txt2vec(self, text, reverse=False, trunc=None):\n","    unknownidx = len(self.word2idx) + 1\n","    sequence = [self.word2idx.get(w, unknownidx) for w in text.split()]\n","    if reverse:\n","      sequence = sequence[::-1]\n","    if type(trunc) is int:\n","      sequence = sequence[:trunc]\n","    return sequence\n","\n","  def vec2txt(self, vec):\n","    unknown = '<UNK>'\n","    text = ' '.join(self.idx2word.get(i, unknown) for i in vec)\n","    return text"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Zh77sv20nGLX"},"source":["# Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686977636084,"user":{"displayName":"Minghao Zhu","userId":"16328474558609043768"},"user_tz":-480},"id":"W9FmCavDm3PP"},"outputs":[],"source":["from typing import Dict, Any, List\n","\n","def load_word_vec(path: str, word2idx: Dict[str, int]):\n","  word_vec = {}\n","  for line in open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore'):\n","    tokens = line.rstrip().split(' ')\n","    # Get the glove word vector\n","    if tokens[0] in word2idx:\n","        word_vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n","  return word_vec\n","\n","def build_embedding_matrix(word2idx: Dict[str, int], embed_dim: int, type):\n","  embedding_matrix_file_name = f'{str(embed_dim)}_{type}_embedding_matrix.dat'\n","\n","  print('loading word vectors...')\n","  embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # 0 = pad token, len + 1 = unk token\n","  fname = f'./embeddings/glove.twitter.27B.{str(embed_dim)}d.txt'\n","  word_vec = load_word_vec(fname, word2idx=word2idx)\n","  print('building embedding_matrix:', embedding_matrix_file_name)\n","\n","  for word, i in word2idx.items():\n","      vec = word_vec.get(word)\n","      if vec is not None:\n","          embedding_matrix[i] = vec\n","\n","  return embedding_matrix\n","\n","class Twitter15Dataset(Dataset):\n","  def __init__(self, split, opt):\n","    super().__init__()\n","    self.opt = opt\n","    self.maxlen = opt.max_seq_len\n","    self.fname = {\n","      'train': './datasets/twitter2015/train.txt',\n","      'dev': './datasets/twitter2015/dev.txt',\n","      'test': './datasets/twitter2015/test.txt'\n","    }\n","    self.img_path = './datasets/IJCAI2019_data/twitter2015_images'\n","    # image preprocessing\n","    self.transform = transforms.Compose([\n","      transforms.RandomCrop(opt.crop_size), #args.crop_size, by default it is set to be 224\n","      transforms.RandomHorizontalFlip(),\n","      transforms.ToTensor(),\n","      transforms.Normalize((0.485, 0.456, 0.406),\n","                           (0.229, 0.224, 0.225))])\n","    self.tokenizer = Tokenizer()\n","    self.init_tokenizer([self.fname['train'], self.fname['dev'], self.fname['test']])\n","    # load once only\n","    if split == 'train':\n","      self.embedding_matrix = build_embedding_matrix(self.tokenizer.word2idx, opt.embed_dim, 'twitter15')\n","    self.data = []\n","    self.load_data(self.fname[split])\n","\n","  def image_process(self, image_path):\n","    image = Image.open(image_path).convert('RGB')\n","    image = self.transform(image)\n","    return image\n","\n","  def show_sample(self, i):\n","    sample = self[i]\n","    for k, v in sample.items():\n","      if k != 'polarity' and type(v) is list:\n","        print(k, v, self.tokenizer.vec2txt(v))\n","      elif k == 'image':\n","        print(k, v.size())\n","      else:\n","        print(k, v)\n","\n","  def collate_fn(self, batch_samples: List[Dict[str, Any]]):\n","    batch = {}\n","    for k in batch_samples[0].keys():\n","      if k == 'polarity':\n","        batch[k] = torch.tensor([b[k] for b in batch_samples], dtype=torch.long)\n","      elif k == 'image':\n","        batch[k] = torch.stack([b[k] for b in batch_samples], dim=0)\n","      elif type(batch_samples[0][k]) is list:\n","        padded_seq = self.tokenizer.pad_sequences([b[k] for b in batch_samples])\n","        batch[k] = torch.tensor(padded_seq, dtype=torch.long)\n","      else:\n","        raise ValueError(f'Unknown data: {k}')\n","    return batch\n","\n","  def __getitem__(self, index):\n","    return self.data[index]\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def init_tokenizer(self, fnames):\n","    text = ''\n","    for fname in fnames:\n","      with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n","        lines = f.readlines()\n","      for i in range(0, len(lines), 4):\n","        text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n","        aspect = lines[i + 1].lower().strip()\n","        text_raw = text_left + \" \" + aspect + \" \" + text_right\n","        text += text_raw + \" \"\n","    self.tokenizer.fit(text.lower())\n","\n","  def load_data(self, fname):\n","    print('--------------'+fname+'---------------')\n","    with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n","      lines = f.readlines()\n","    count = 0\n","    for i in tqdm.tqdm(range(0, len(lines), 4)):\n","      text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n","      aspect = lines[i + 1].lower().strip()\n","      label = lines[i + 2].strip()\n","      image_id = lines[i + 3].strip()\n","\n","      text_left_for_fusion = text_left + \" <e>\"\n","      text_right_for_fusion = \"</e> \" + text_right\n","\n","      text_raw_indices = self.tokenizer.txt2vec(text_left + \" \" + aspect + \" \" + text_right)\n","      text_raw_without_aspect_indices = self.tokenizer.txt2vec(text_left + \" \" + text_right)\n","\n","      text_left_indicator = self.tokenizer.txt2vec(text_left_for_fusion)\n","      text_left_with_aspect_indices = self.tokenizer.txt2vec(text_left + \" \" + aspect)\n","\n","      text_right_indicator = self.tokenizer.txt2vec(text_right_for_fusion)\n","      text_right_with_aspect_indices = self.tokenizer.txt2vec(\" \" + aspect + \" \" + text_right, reverse=True)\n","\n","      aspect_indices = self.tokenizer.txt2vec(aspect)\n","      label = int(label) + 1\n","\n","      image_name = image_id\n","      image_path = os.path.join(self.img_path, image_name)\n","\n","      if not os.path.exists(image_path):\n","          print(image_path)\n","      try:\n","          image = self.image_process(image_path)\n","      except:\n","          count += 1\n","          #print('image has problem!')\n","          image_path_fail = os.path.join(self.img_path, '17_06_4705.jpg')\n","          image = self.image_process(image_path_fail)\n","\n","      data = {\n","            'text_raw_indices': text_raw_indices,\n","            'text_raw_without_aspect_indices': text_raw_without_aspect_indices,\n","            # 'text_left_indices': text_left_indices,\n","            'text_left_indicator': text_left_indicator[:self.maxlen],\n","            # 'text_left_with_aspect_indices': text_left_with_aspect_indices,\n","            # 'text_right_indices': text_right_indices,\n","            'text_right_indicator': text_right_indicator[:self.maxlen],\n","            # 'text_right_with_aspect_indices': text_right_with_aspect_indices,\n","            'aspect_indices': aspect_indices[:self.maxlen],\n","            'polarity': label,\n","            'image': image,\n","        }\n","      self.data.append(data)\n","    print('the number of problematic samples: '+ str(count))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jbhpa4TIfdm2"},"source":["# Dataset debug单元测试"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686977636084,"user":{"displayName":"Minghao Zhu","userId":"16328474558609043768"},"user_tz":-480},"id":"bBKx7uCF-iAt"},"outputs":[],"source":["# from argparse import Namespace\n","# opt = Namespace()\n","# opt.crop_size = 224\n","# opt.embed_dim = 100\n","# opt.max_seq_len=100\n","# dataset = Twitter15Dataset('dev', opt)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686977636085,"user":{"displayName":"Minghao Zhu","userId":"16328474558609043768"},"user_tz":-480},"id":"kojehn4f_QHw"},"outputs":[],"source":["# dataset.show_sample(1)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YmIMgYgHJdaO"},"source":["## Model"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686977636085,"user":{"displayName":"Minghao Zhu","userId":"16328474558609043768"},"user_tz":-480},"id":"5OIjrYAsJbz2"},"outputs":[],"source":["from layers.lstm import MyLSTM\n","from layers.attention import Attention\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class MMFUSION(nn.Module):\n","    def __init__(self, embedding_matrix, opt):\n","        super(MMFUSION, self).__init__()\n","        self.opt = opt\n","        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n","\n","        self.lstm_aspect = MyLSTM(opt.embed_dim, opt.hidden_dim, dropout = opt.dropout_rate)\n","        self.lstm_l = MyLSTM(opt.embed_dim, opt.hidden_dim, dropout = opt.dropout_rate)\n","        self.lstm_r = MyLSTM(opt.embed_dim, opt.hidden_dim, dropout = opt.dropout_rate)\n","\n","        self.attention_l = Attention(opt.hidden_dim, dropout=opt.dropout_rate)\n","        self.attention_r = Attention(opt.hidden_dim, dropout=opt.dropout_rate)\n","\n","        self.ltext2hidden = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n","        self.laspect2hidden = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n","\n","        self.rtext2hidden = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n","        self.raspect2hidden = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n","\n","        self.dropout = nn.Dropout(self.opt.dropout_rate)\n","\n","        self.aspect2text = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n","        self.vismap2text = nn.Linear(2048, opt.hidden_dim)\n","        self.vis2text = nn.Linear(2048, opt.hidden_dim)\n","\n","        self.gate = nn.Linear(2048+4*opt.hidden_dim, opt.hidden_dim)\n","\n","        self.madality_attetion = nn.Linear(opt.hidden_dim,1)\n","\n","\n","        self.text2hiddenvis = nn.Linear(opt.hidden_dim * 4, opt.hidden_dim)\n","        self.vis2hiddenvis = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n","\n","        self.dense_2 = nn.Linear(opt.hidden_dim*2, opt.polarities_dim)\n","        self.dense_3 = nn.Linear(opt.hidden_dim*3, opt.polarities_dim)\n","        self.dense_4 = nn.Linear(opt.hidden_dim*4, opt.polarities_dim)\n","        self.dense_5 = nn.Linear(opt.hidden_dim*5, opt.polarities_dim)\n","        self.dense_6 = nn.Linear(opt.hidden_dim*6, opt.polarities_dim)\n","        self.dense_10 = nn.Linear(opt.hidden_dim*10, opt.polarities_dim)\n","\n","\n","    def attention_linear(self, text, converted_vis_embed_map, vis_embed_map):\n","        #text: batch_size, hidden_dim; converted_vis_embed_map: batch_size, keys_number,embed_size; vis_embed_map: batch_size, keys_number, 2048\n","        keys_size = converted_vis_embed_map.size(1)\n","        text = text.unsqueeze(1).expand(-1, keys_size, -1)#batch_size, keys_number,hidden_dim\n","        attention_inputs = torch.tanh(text + converted_vis_embed_map)\n","        #attention_inputs = F.dropout( attention_inputs )\n","        att_weights = self.madality_attetion(attention_inputs).squeeze(2) #batch_size, keys_number\n","        att_weights = F.softmax(att_weights, dim=-1).view(-1,1,49) #batch_size, 1, keys_number\n","\n","        att_vector = torch.bmm(att_weights, vis_embed_map).view(-1, 2048) #batch_size, 2048\n","        return att_vector, att_weights\n","\n","    def low_rank_pooling(self, final, aspect, text2hidden, aspect2hidden):\n","        text = torch.tanh(text2hidden(final))\n","        aspect = torch.tanh(aspect2hidden(aspect))\n","        text_aspect_inter = torch.mul(text, aspect)\n","        output = torch.cat((text_aspect_inter, final), dim=-1)\n","        return output\n","\n","    def forward(self, inputs, visual_embeds_global, visual_embeds_att, att_mode):\n","        x_l, x_r, aspect_indices = inputs[0], inputs[1], inputs[2]\n","        ori_x_l_len = torch.sum(x_l != 0, dim=-1)\n","        ori_x_r_len = torch.sum(x_r != 0, dim=-1)\n","        ori_aspect_len = torch.sum(aspect_indices != 0, dim=-1)\n","\n","        aspect = self.embed(aspect_indices)\n","        aspect_lstm, (_, _) = self.lstm_aspect(aspect, ori_aspect_len)\n","        aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n","        sum_aspect = torch.sum(aspect_lstm, dim=1)\n","        avg_aspect = torch.div(sum_aspect, aspect_len.view(aspect_len.size(0), 1))\n","\n","        # obtain the lstm hidden states for the left context and the right context respectively\n","        x_l, x_r = self.embed(x_l), self.embed(x_r)\n","        l_context, (_, _) = self.lstm_l(x_l, ori_x_l_len)\n","        r_context, (_, _) = self.lstm_r(x_r, ori_x_r_len)\n","\n","        converted_vis_embed = self.vis2text(torch.tanh(visual_embeds_global))\n","\n","        if att_mode == 'ESTR': # apply aspect words to attend the left and right contexts\n","            l_mid, l_att = self.attention_l(l_context, avg_aspect, ori_x_l_len)\n","            r_mid, r_att = self.attention_r(r_context, avg_aspect, ori_x_r_len)\n","            l_final = l_mid.squeeze(dim=1)\n","            r_final = r_mid.squeeze(dim=1)\n","\n","            # low-rank pooling\n","            l_output = self.low_rank_pooling(l_final, avg_aspect, self.laspect2hidden, self.laspect2hidden)\n","            r_output = self.low_rank_pooling(r_final, avg_aspect, self.raspect2hidden, self.raspect2hidden)\n","\n","            text_representation = torch.cat((l_output, r_output), dim=-1)\n","            x = text_representation\n","            x = self.dropout(x)\n","            out = self.dense_4(x)\n","            #\"\"\"\n","            return out\n","\n","        elif att_mode == 'ResTarget': #only use image and aspect words\n","            vis_embed_map = visual_embeds_att.view(-1, 2048, 49).permute(0, 2, 1)#self.batch_size, 49, 2048\n","            converted_vis_embed_map = self.vismap2text(vis_embed_map) #self.batch_size, 49, embed\n","            converted_aspect = self.aspect2text(avg_aspect)\n","\n","            #att_vector: batch_size, 2048\n","            att_vector, att_weights = self.attention_linear(converted_aspect, converted_vis_embed_map, vis_embed_map)\n","            converted_att_vis_embed = self.vis2text(torch.tanh(att_vector))\n","            x = torch.cat((avg_aspect, converted_att_vis_embed), dim=-1)\n","            out = self.dense_2(x)\n","            return out\n","\n","        elif att_mode == 'without_ESVR': # \"text\" mode concatenated with image\n","            l_mid, l_att = self.attention_l(l_context, avg_aspect, ori_x_l_len)\n","            r_mid, r_att = self.attention_r(r_context, avg_aspect, ori_x_r_len)\n","            l_final = l_mid.squeeze(dim=1)\n","            r_final = r_mid.squeeze(dim=1)\n","\n","            #\"\"\"\n","            # low-rank pooling\n","            l_output = self.low_rank_pooling(l_final, avg_aspect, self.laspect2hidden, self.laspect2hidden)\n","            r_output = self.low_rank_pooling(r_final, avg_aspect, self.raspect2hidden, self.raspect2hidden)\n","            text_representation = torch.cat((l_output, r_output), dim=-1)\n","\n","            x = torch.cat((text_representation, converted_vis_embed), dim=-1)\n","            x = self.dropout(x)\n","            out = self.dense_5(x)\n","            return out\n","        elif att_mode == 'without_gate':  # \"text\" mode concatenated with attention-based image\n","            l_mid, l_att = self.attention_l(l_context, avg_aspect, ori_x_l_len)\n","            r_mid, r_att = self.attention_r(r_context, avg_aspect, ori_x_r_len)\n","            l_final = l_mid.squeeze(dim=1)\n","            r_final = r_mid.squeeze(dim=1)\n","\n","            #\"\"\"\n","            # low-rank pooling\n","            l_output = self.low_rank_pooling(l_final, avg_aspect, self.laspect2hidden, self.laspect2hidden)\n","            r_output = self.low_rank_pooling(r_final, avg_aspect, self.raspect2hidden, self.raspect2hidden)\n","            text_representation = torch.cat((l_output, r_output), dim=-1)\n","\n","            vis_embed_map = visual_embeds_att.view(-1, 2048, 49).permute(0, 2, 1)  # self.batch_size, 49, 2048\n","            converted_vis_embed_map = self.vismap2text(vis_embed_map)  # self.batch_size, 49, embed\n","            converted_aspect = self.aspect2text(avg_aspect)\n","\n","            # att_vector: batch_size, 2048\n","            att_vector, att_weights = self.attention_linear(converted_aspect, converted_vis_embed_map, vis_embed_map)\n","            converted_att_vis_embed = self.vis2text(torch.tanh(att_vector))\n","\n","            x = torch.cat((text_representation, converted_att_vis_embed), dim=-1)\n","            x = self.dropout(x)\n","\n","            out = self.dense_5(x)\n","            return out\n","        elif att_mode == 'without_MFL':\n","            l_mid, l_att = self.attention_l(l_context, avg_aspect, ori_x_l_len)\n","            r_mid, r_att = self.attention_r(r_context, avg_aspect, ori_x_r_len)\n","            l_final = l_mid.squeeze(dim=1)\n","            r_final = r_mid.squeeze(dim=1)\n","\n","            # low-rank pooling\n","            l_output = self.low_rank_pooling(l_final, avg_aspect, self.laspect2hidden, self.laspect2hidden)\n","            r_output = self.low_rank_pooling(r_final, avg_aspect, self.raspect2hidden, self.raspect2hidden)\n","            text_representation = torch.cat((l_output, r_output), dim=-1)\n","\n","\n","            # apply entity-based attention mechanism to obtain different image representations\n","            vis_embed_map = visual_embeds_att.view(-1, 2048, 49).permute(0, 2, 1)  # self.batch_size, 49, 2048\n","            converted_vis_embed_map = self.vismap2text(vis_embed_map)  # self.batch_size, 49, embed\n","            converted_aspect = self.aspect2text(avg_aspect)\n","\n","            # att_vector: batch_size, 2048\n","            att_vector, att_weights = self.attention_linear(converted_aspect, converted_vis_embed_map, vis_embed_map)\n","            converted_att_vis_embed = self.vis2text(torch.tanh(att_vector))  # att_vector: batch_size, hidden_dim\n","\n","            merge_representation = torch.cat((text_representation, att_vector), dim=-1)\n","            gate_value = torch.sigmoid(self.gate(merge_representation))  # batch_size, hidden_dim\n","            gated_converted_att_vis_embed = torch.mul(gate_value, converted_att_vis_embed)\n","            #gated_converted_att_vis_embed = self.dropout(gated_converted_att_vis_embed)\n","\n","            vis_output = gated_converted_att_vis_embed\n","            # vis_output = gated_converted_att_vis_embed + text_vis_inter\n","            x = torch.cat((text_representation, vis_output), dim=-1)\n","            x = self.dropout(x)\n","            out = self.dense_5(x)\n","\n","            return out\n","        elif att_mode == 'ESAFN':  # \"text\" mode concatenated with gated attention-based image\n","            l_mid, l_att = self.attention_l(l_context, avg_aspect, ori_x_l_len)\n","            r_mid, r_att = self.attention_r(r_context, avg_aspect, ori_x_r_len)\n","            l_final = l_mid.squeeze(dim=1)\n","            r_final = r_mid.squeeze(dim=1)\n","\n","            #\"\"\"\n","            # low-rank pooling\n","            l_output = self.low_rank_pooling(l_final, avg_aspect, self.laspect2hidden, self.laspect2hidden)\n","            r_output = self.low_rank_pooling(r_final, avg_aspect, self.raspect2hidden, self.raspect2hidden)\n","            text_representation = torch.cat((l_output, r_output), dim=-1)\n","            #\"\"\"\n","\n","            # apply entity-based attention mechanism to obtain different image representations\n","            vis_embed_map = visual_embeds_att.view(-1, 2048, 49).permute(0, 2, 1)  # self.batch_size, 49, 2048\n","            converted_vis_embed_map = self.vismap2text(vis_embed_map)  # self.batch_size, 49, embed\n","            converted_aspect = self.aspect2text(avg_aspect)\n","\n","            # att_vector: batch_size, 2048\n","            att_vector, att_weights = self.attention_linear(converted_aspect, converted_vis_embed_map, vis_embed_map)\n","            converted_att_vis_embed = self.vis2text(torch.tanh(att_vector))  # att_vector: batch_size, hidden_dim\n","\n","            merge_representation = torch.cat((text_representation, att_vector), dim=-1)\n","            gate_value = torch.sigmoid(self.gate(merge_representation))  # batch_size, hidden_dim\n","            gated_converted_att_vis_embed = torch.mul(gate_value, converted_att_vis_embed)\n","            #gated_converted_att_vis_embed = self.dropout(gated_converted_att_vis_embed)\n","\n","            #\"\"\"\n","            text_vis = torch.tanh(self.text2hiddenvis(text_representation))  # batch_size, hidde_dim\n","            vis_vis = torch.tanh(self.vis2hiddenvis(gated_converted_att_vis_embed))  # batch_size, hidden_dim\n","\n","            text_vis_inter = torch.mul(text_vis, vis_vis)\n","            vis_output = torch.cat((gated_converted_att_vis_embed, text_vis_inter), dim=-1)\n","\n","            x = torch.cat((text_representation, vis_output), dim=-1)\n","            x = self.dropout(x)\n","            out = self.dense_6(x)\n","            #\"\"\"\n","\n","            return out"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jVJRznHmfiJz"},"source":["# Trainer"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686977636085,"user":{"displayName":"Minghao Zhu","userId":"16328474558609043768"},"user_tz":-480},"id":"MsXvOs7nEnEC"},"outputs":[],"source":["from resnet.resnet import resnet152\n","from sklearn.metrics import classification_report, accuracy_score, f1_score\n","import gc\n","class Trainer:\n","  def __init__(self, opt):\n","    self.opt = opt\n","    print('> training arguments:')\n","    for arg in vars(opt):\n","        print('>>> {0}: {1}'.format(arg, getattr(opt, arg)))\n","\n","    # use class attribute to avoid repeat load\n","    # def __init__(self, split, opt):\n","    self.train_dataset = Twitter15Dataset('train', opt)\n","    self.dev_dataset = Twitter15Dataset('dev', opt)\n","    self.test_dataset = Twitter15Dataset('test', opt)\n","\n","    self.train_data_loader = DataLoader(dataset=self.train_dataset, batch_size=opt.batch_size, shuffle=True, collate_fn=self.train_dataset.collate_fn)\n","    self.dev_data_loader = DataLoader(dataset=self.dev_dataset, batch_size=opt.batch_size, shuffle=False, collate_fn=self.dev_dataset.collate_fn)\n","    self.test_data_loader = DataLoader(dataset=self.test_dataset, batch_size=opt.batch_size, shuffle=False, collate_fn=self.test_dataset.collate_fn)\n","\n","    resnet = resnet152()\n","    resnet.load_state_dict(torch.load(os.path.join('resnet/resnet152.pth')))\n","    self.encoder = myResnet(resnet, self.opt.att_mode, self.opt.device).to(device)\n","    if not opt.fine_tune_cnn:\n","      self.encoder.requires_grad_(False)\n","    self.model = MMFUSION(self.train_dataset.embedding_matrix, opt).to(device)\n","    self.reset_parameters()\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","  def reset_parameters(self):\n","    n_trainable_params, n_nontrainable_params = 0, 0\n","    for n, p in self.model.named_parameters():\n","      n_params = torch.prod(torch.tensor(p.shape))\n","      if p.requires_grad:\n","        n_trainable_params += n_params\n","        if len(p.shape) > 1:\n","          torch.nn.init.xavier_uniform_(p)\n","      else:\n","        n_nontrainable_params += n_params\n","        print(n)\n","    print('n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n","\n","  def run_model(self, sample_batched):\n","    inputs = [sample_batched[col].to(device) for col in self.opt.inputs_cols]\n","    images = sample_batched['image'].to(device)\n","\n","    imgs_f, _, img_att = self.encoder(images)\n","    outputs = self.model(inputs, imgs_f, img_att, self.opt.att_mode)\n","    return outputs\n","\n","  def record_track_list(self, track_list, loss, outputs, targets):\n","    track_list['losses'].append(loss.item())\n","    track_list['preds'].extend(outputs.argmax(-1).tolist())\n","    track_list['targets'].extend(targets.tolist())\n","\n","  @torch.no_grad()\n","  def evaluate(self, eval_on='dev', print_results=True):\n","    self.model.eval()\n","    self.encoder.eval()\n","    print(f'Start evaluation on {eval_on}')\n","\n","    criterion = nn.CrossEntropyLoss()\n","    track_list = {\n","        'losses': [],\n","        'preds': [],\n","        'targets': [],\n","    }\n","\n","    if eval_on == 'dev':\n","      data_loader = self.dev_data_loader\n","    elif eval_on == 'test':\n","      data_loader = self.test_data_loader\n","    else:\n","      raise ValueError(eval_on)\n","\n","    for i_batch, sample_batched in enumerate(data_loader):\n","      targets = sample_batched['polarity'].to(device)\n","      outputs = self.run_model(sample_batched)\n","      loss = criterion(outputs, targets)\n","      self.record_track_list(track_list, loss, outputs, targets)\n","\n","    if print_results:\n","      print(classification_report(y_pred=track_list['preds'],\n","                                  y_true=track_list['targets'],\n","                                  digits=4))\n","    return track_list\n","\n","\n","  def train(self):\n","    criterion = nn.CrossEntropyLoss()\n","    params = [p for p in self.model.parameters() if p.requires_grad] + \\\n","             [p for p in self.encoder.parameters() if p.requires_grad]\n","    optimizer = torch.optim.AdamW(params, lr=self.opt.learning_rate)\n","\n","    max_dev_acc = 0.\n","    max_test_acc = max_test_f1 = 0.\n","    global_step = 0\n","    track_list = {\n","        'losses': [],\n","        'preds': [],\n","        'targets': [],\n","    }\n","\n","    for epoch in range(self.opt.num_epoch):\n","      print('>' * 100)\n","      print('epoch: ', epoch)\n","      n_correct, n_total = 0, 0\n","\n","      for i_batch, sample_batched in enumerate(tqdm.tqdm(self.train_data_loader)):\n","        self.model.train()\n","        self.encoder.train()\n","        global_step += 1\n","        optimizer.zero_grad()\n","\n","        targets = sample_batched['polarity'].to(device)\n","        outputs = self.run_model(sample_batched)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(params, 5.)\n","        optimizer.step()\n","        self.record_track_list(track_list, loss, outputs, targets)\n","\n","        if global_step % self.opt.log_step == 0:\n","          print('\\n===== Metrics on train: =====')\n","          print(f'Loss: {sum(track_list[\"losses\"]) / len(track_list[\"losses\"])}')\n","          # print(classification_report(y_pred=track_list['preds'],\n","          #                             y_true=track_list['targets'],\n","          #                             digits=4))\n","\n","          track_list_dev = self.evaluate('dev', False)\n","\n","          print('\\n===== Metrics on dev: =====')\n","          print(f'Loss: {sum(track_list_dev[\"losses\"]) / len(track_list_dev[\"losses\"])}')\n","          print(f\"Acc: {accuracy_score(y_pred=track_list_dev['preds'], y_true=track_list_dev['targets'])}\")\n","          print(f\"Macro_f1: {f1_score(y_pred=track_list_dev['preds'], y_true=track_list_dev['targets'], average='macro')}\")\n","          # print(classification_report(y_pred=track_list_dev['preds'],\n","          #                             y_true=track_list_dev['targets'],\n","          #                             digits=4))\n","\n","          track_list_test = self.evaluate('test', False)\n","\n","          print('\\n===== Metrics on test: =====')\n","          print(f'Loss: {sum(track_list_test[\"losses\"]) / len(track_list_test[\"losses\"])}')\n","          print(f\"Acc: {accuracy_score(y_pred=track_list_test['preds'], y_true=track_list_test['targets'])}\")\n","          print(f\"Macro_f1: {f1_score(y_pred=track_list_test['preds'], y_true=track_list_test['targets'], average='macro')}\")\n","          # print(classification_report(y_pred=track_list_test['preds'],\n","          #                             y_true=track_list_test['targets'],\n","          #                             digits=4))\n","\n","          dev_acc = accuracy_score(y_pred=track_list_dev['preds'],\n","                                  y_true=track_list_dev['targets'],)\n","\n","          if dev_acc > max_dev_acc:\n","            max_dev_acc = dev_acc\n","            max_test_acc = accuracy_score(y_pred=track_list_test['preds'], y_true=track_list_test['targets'])\n","            max_test_f1 = f1_score(y_pred=track_list_test['preds'], y_true=track_list_test['targets'], average='macro')\n","            print(f\"Save the best model with dev_acc = {dev_acc}\")\n","            torch.save({\n","                'state_dict': self.model.state_dict(),\n","                'encode_dict': self.encoder.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, f'best_checkpoint_epoch{epoch}.pth')\n","      print(f\"max_test_acc={max_test_acc}, max_test_f1={max_test_f1}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8OpcIdnFfl3k","outputId":"2e04b6db-ed20-4e42-bfc3-f5a3455331b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["> training arguments:\n",">>> crop_size: 224\n",">>> embed_dim: 100\n",">>> hidden_dim: 100\n",">>> fine_tune_cnn: False\n",">>> dropout_rate: 0.5\n",">>> learning_rate: 0.001\n",">>> num_epoch: 6\n",">>> batch_size: 10\n",">>> att_mode: ESTR\n",">>> device: cuda\n",">>> tfn: False\n",">>> polarities_dim: 3\n",">>> max_seq_len: 100\n",">>> log_step: 50\n",">>> inputs_cols: ['text_left_indicator', 'text_right_indicator', 'aspect_indices']\n","loading word vectors...\n","building embedding_matrix: 100_twitter15_embedding_matrix.dat\n","--------------./datasets/twitter2015/train.txt---------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3179/3179 [00:25<00:00, 125.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["the number of problematic samples: 82\n","--------------./datasets/twitter2015/dev.txt---------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1122/1122 [00:09<00:00, 121.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["the number of problematic samples: 30\n","--------------./datasets/twitter2015/test.txt---------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1037/1037 [00:07<00:00, 136.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["the number of problematic samples: 27\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"name":"stdout","output_type":"stream","text":["embed.weight\n","n_trainable_params: 1207319, n_nontrainable_params: 1290500\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch:  0\n"]},{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/318 [00:00<?, ?it/s]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 15%|█▌        | 49/318 [00:09<00:26, 10.18it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.9670945143699646\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.9153892656343173\n","Acc: 0.6096256684491979\n","Macro_f1: 0.30543837795116807\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.898076495872094\n","Acc: 0.5969141755062681\n","Macro_f1: 0.2936680517082179\n","Save the best model with dev_acc = 0.6096256684491979\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 31%|███       | 99/318 [00:32<00:20, 10.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.9419023025035859\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.9013284645249359\n","Acc: 0.6060606060606061\n","Macro_f1: 0.41535115968772923\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 31%|███▏      | 100/318 [00:50<11:53,  3.28s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 32%|███▏      | 102/318 [00:50<07:57,  2.21s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.8744753146400819\n","Acc: 0.6113789778206364\n","Macro_f1: 0.41320572290629737\n"]},{"name":"stderr","output_type":"stream","text":[" 47%|████▋     | 148/318 [00:54<00:16, 10.33it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.9249367839097977\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.8030256265032608\n","Acc: 0.6550802139037433\n","Macro_f1: 0.4216982334964237\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 47%|████▋     | 148/318 [01:07<00:16, 10.33it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7823905878915236\n","Acc: 0.6721311475409836\n","Macro_f1: 0.4384518275338875\n","Save the best model with dev_acc = 0.6550802139037433\n"]},{"name":"stderr","output_type":"stream","text":[" 47%|████▋     | 150/318 [01:14<08:19,  2.98s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 63%|██████▎   | 199/318 [01:19<00:11, 10.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.8849504789710045\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7869640183923519\n","Acc: 0.64349376114082\n","Macro_f1: 0.42896760795339045\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n","<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 63%|██████▎   | 201/318 [01:37<05:30,  2.83s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7607840759536395\n","Acc: 0.6692381870781099\n","Macro_f1: 0.4660059086627583\n"]},{"name":"stderr","output_type":"stream","text":[" 78%|███████▊  | 249/318 [01:42<00:07,  9.73it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.8631223950386048\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7764968372947347\n","Acc: 0.6595365418894831\n","Macro_f1: 0.43252355978861495\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7570441170380666\n","Acc: 0.6750241080038573\n","Macro_f1: 0.4380849299430283\n","Save the best model with dev_acc = 0.6595365418894831\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▊  | 250/318 [02:01<05:49,  5.14s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 94%|█████████▎| 298/318 [02:05<00:01, 10.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.8479341727495193\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7533417214862014\n","Acc: 0.6666666666666666\n","Macro_f1: 0.4824773162407392\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 94%|█████████▎| 298/318 [02:17<00:01, 10.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7129008322954178\n","Acc: 0.7010607521697203\n","Macro_f1: 0.5296060390781135\n","Save the best model with dev_acc = 0.6666666666666666\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 300/318 [02:24<00:51,  2.88s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n","100%|██████████| 318/318 [02:26<00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["max_test_acc=0.7010607521697203, max_test_f1=0.5296060390781135\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch:  1\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 30/318 [00:02<00:27, 10.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.8235619186503547\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7864071697787901\n","Acc: 0.661319073083779\n","Macro_f1: 0.4163527044211514\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 10%|█         | 32/318 [00:21<13:22,  2.81s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7559776379225346\n","Acc: 0.6711668273866924\n","Macro_f1: 0.4228227584862558\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 25%|██▌       | 80/318 [00:25<00:22, 10.37it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.8084261029213667\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7631373608534315\n","Acc: 0.6586452762923352\n","Macro_f1: 0.5293446756861391\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 26%|██▌       | 82/318 [00:44<11:33,  2.94s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 26%|██▌       | 83/318 [00:44<09:28,  2.42s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7582512874729358\n","Acc: 0.6682738669238187\n","Macro_f1: 0.5485564014343761\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|████      | 131/318 [00:48<00:17, 10.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7980495541625553\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.750130610787763\n","Acc: 0.6648841354723708\n","Macro_f1: 0.4865342638644106\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 42%|████▏     | 132/318 [01:06<10:06,  3.26s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 42%|████▏     | 133/318 [01:07<08:05,  2.63s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7043721072662336\n","Acc: 0.7029893924783028\n","Macro_f1: 0.5466378000111476\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 180/318 [01:11<00:13, 10.48it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7910173921585083\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.738279866697514\n","Acc: 0.661319073083779\n","Macro_f1: 0.5264426168741365\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 57%|█████▋    | 182/318 [01:29<06:19,  2.79s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 58%|█████▊    | 184/318 [01:29<04:25,  1.98s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7086128901976806\n","Acc: 0.703953712632594\n","Macro_f1: 0.58473411154345\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 231/318 [01:34<00:09,  9.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7850897190245715\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7241386710542493\n","Acc: 0.6657754010695187\n","Macro_f1: 0.532101584446823\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 73%|███████▎  | 232/318 [01:52<07:53,  5.51s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 73%|███████▎  | 233/318 [01:52<05:30,  3.88s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.6946076426941615\n","Acc: 0.7087753134040502\n","Macro_f1: 0.5767688562863749\n"]},{"name":"stderr","output_type":"stream","text":[" 88%|████████▊ | 280/318 [01:57<00:03, 10.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7773364414771398\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7361252735805722\n","Acc: 0.6657754010695187\n","Macro_f1: 0.5280332248142434\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 89%|████████▊ | 282/318 [02:15<01:40,  2.80s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 89%|████████▉ | 284/318 [02:15<01:07,  1.99s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.712865736335516\n","Acc: 0.7174541947926711\n","Macro_f1: 0.6072724546408756\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 318/318 [02:19<00:00,  2.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["max_test_acc=0.7010607521697203, max_test_f1=0.5296060390781135\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch:  2\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 12/318 [00:01<00:28, 10.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7735127216119032\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7265827965024299\n","Acc: 0.6773618538324421\n","Macro_f1: 0.519167943689482\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n","  4%|▍         | 12/318 [00:12<00:28, 10.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.6884776803736503\n","Acc: 0.7145612343297975\n","Macro_f1: 0.5597297791569548\n","Save the best model with dev_acc = 0.6773618538324421\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 14/318 [00:19<15:52,  3.13s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 20%|█▉        | 63/318 [00:24<00:24, 10.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7626343347132206\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7516711469640774\n","Acc: 0.6639928698752228\n","Macro_f1: 0.5311173182458644\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 20%|██        | 64/318 [00:42<14:00,  3.31s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 21%|██        | 66/318 [00:42<09:19,  2.22s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7024179780139372\n","Acc: 0.7107039537126326\n","Macro_f1: 0.5864940113025824\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 113/318 [00:47<00:20,  9.88it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7524060492714246\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7834237273287984\n","Acc: 0.6559714795008913\n","Macro_f1: 0.5693805145490706\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 36%|███▌      | 114/318 [01:05<12:41,  3.73s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 36%|███▌      | 115/318 [01:05<09:53,  2.92s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7143452442609347\n","Acc: 0.6885245901639344\n","Macro_f1: 0.5996177521982684\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████▏    | 163/318 [01:10<00:14, 10.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7475429455563426\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7681232088171275\n","Acc: 0.661319073083779\n","Macro_f1: 0.5227570470017728\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 52%|█████▏    | 164/318 [01:28<08:24,  3.28s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 52%|█████▏    | 165/318 [01:28<06:43,  2.64s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.7024466753579103\n","Acc: 0.7174541947926711\n","Macro_f1: 0.6053283093243013\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 212/318 [01:33<00:10, 10.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7423529245397624\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7362536153039047\n","Acc: 0.6818181818181818\n","Macro_f1: 0.5483307538277759\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.6840448860938733\n","Acc: 0.703953712632594\n","Macro_f1: 0.586644926589122\n","Save the best model with dev_acc = 0.6818181818181818\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 214/318 [01:52<05:01,  2.89s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 83%|████████▎ | 263/318 [01:56<00:05,  9.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on train: =====\n","Loss: 0.7335537601510683\n","Start evaluation on dev\n","\n","===== Metrics on dev: =====\n","Loss: 0.7309911174584279\n","Acc: 0.6934046345811051\n","Macro_f1: 0.5865894997022782\n","Start evaluation on test\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"]},{"name":"stdout","output_type":"stream","text":["\n","===== Metrics on test: =====\n","Loss: 0.6968628026258487\n","Acc: 0.7261330761812922\n","Macro_f1: 0.6329748030851531\n","Save the best model with dev_acc = 0.6934046345811051\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 264/318 [02:15<04:47,  5.32s/it]<ipython-input-9-efa0b40ec306>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  aspect_len = torch.tensor(ori_aspect_len, dtype=torch.float).to(self.opt.device)\n"," 87%|████████▋ | 278/318 [02:17<00:13,  2.97it/s]"]}],"source":["from argparse import Namespace\n","import numpy as np\n","opt = Namespace()\n","opt.crop_size = 224\n","opt.embed_dim = 100\n","opt.hidden_dim = 100\n","opt.fine_tune_cnn = False\n","opt.dropout_rate = 0.5\n","opt.learning_rate = 0.001\n","opt.num_epoch = 6\n","opt.batch_size = 10\n","opt.att_mode = 'ESTR'\n","opt.device = 'cuda'\n","opt.polarities_dim = 3\n","opt.max_seq_len = 100\n","opt.log_step = 50\n","opt.inputs_cols = ['text_left_indicator', 'text_right_indicator', 'aspect_indices']\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","trainer = Trainer(opt)\n","trainer.train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
